{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.utils import set_seed\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = pd.read_csv(\"../data/avila/avila-tr.txt\", header=None)\n",
    "ds_test = pd.read_csv(\"../data/avila/avila-ts.txt\", header=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Required models --\n",
    "\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# LVQ\n",
    "from sklvq import GLVQ\n",
    "\n",
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Ensemble\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier\n",
    "\n",
    "# -- Optional models --\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Auxiliar functions --\n",
    "\n",
    "from time import time\n",
    "\n",
    "\n",
    "# Compute Training Time\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.start = time()\n",
    "\n",
    "    def __call__(self):\n",
    "        return time() - self.start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "\n",
    "# import label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def get_model_name(model):\n",
    "    if model.__class__.__name__ == \"VotingClassifier\":\n",
    "        return (\n",
    "            model.__class__.__name__\n",
    "            + \"_\"\n",
    "            + \"_\".join([estimator[0] for estimator in model.estimators])\n",
    "        )\n",
    "    return model.__class__.__name__\n",
    "\n",
    "\n",
    "# Compute metrics\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, average=\"macro\", zero_division=1),\n",
    "        \"recall\": recall_score(y_true, y_pred, average=\"macro\", zero_division=1),\n",
    "        \"f1\": f1_score(y_true, y_pred, average=\"macro\", zero_division=1),\n",
    "    }\n",
    "\n",
    "\n",
    "# Split data\n",
    "X_train = ds_train.iloc[:, :-1]\n",
    "y_train = ds_train.iloc[:, -1]\n",
    "X_test = ds_test.iloc[:, :-1]\n",
    "y_test = ds_test.iloc[:, -1]\n",
    "\n",
    "label_encoder = LabelEncoder().fit(y_train)\n",
    "y_train = label_encoder.transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model:  KNeighborsClassifier\n",
      "Training model:  GLVQ\n",
      "Training model:  SVC\n",
      "Training model:  DecisionTreeClassifier\n",
      "Training model:  RandomForestClassifier\n",
      "Training model:  XGBClassifier\n",
      "Training model:  MLPClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matheus\\Documents\\Git\\machine_learning_ufpe_2023_2\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model:  VotingClassifier_dt_knn_rf\n",
      "Training model:  VotingClassifier_mlp1_mlp2_mlp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matheus\\Documents\\Git\\machine_learning_ufpe_2023_2\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Matheus\\Documents\\Git\\machine_learning_ufpe_2023_2\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 50.4 s\n",
      "Wall time: 50.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matheus\\Documents\\Git\\machine_learning_ufpe_2023_2\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Best models of a pre-run are chosen to ensemble\n",
    "models = [\n",
    "    KNeighborsClassifier(),\n",
    "    GLVQ(),\n",
    "    SVC(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    XGBClassifier(),\n",
    "    MLPClassifier(),\n",
    "    VotingClassifier(\n",
    "        estimators=[\n",
    "            (\"dt\", DecisionTreeClassifier()),\n",
    "            (\"knn\", KNeighborsClassifier()),\n",
    "            (\"rf\", RandomForestClassifier()),\n",
    "        ],\n",
    "        voting=\"hard\",\n",
    "    ),\n",
    "    VotingClassifier(\n",
    "        estimators=[\n",
    "            (\"mlp1\", MLPClassifier()),\n",
    "            (\"mlp2\", MLPClassifier()),\n",
    "            (\"mlp3\", MLPClassifier()),\n",
    "        ],\n",
    "        voting=\"hard\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "models_results = {get_model_name(model): {} for model in models}\n",
    "\n",
    "for model in models:\n",
    "    print(\"Training model: \", get_model_name(model))\n",
    "\n",
    "    timer = Timer()\n",
    "    model.fit(X_train, y_train)\n",
    "    metrics = compute_metrics(y_test, model.predict(X_test))\n",
    "    metrics[\"fit_time\"] = timer()\n",
    "\n",
    "    # add new name to dataframe\n",
    "    models_results[get_model_name(model)] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.749545</td>\n",
       "      <td>0.809206</td>\n",
       "      <td>0.665667</td>\n",
       "      <td>0.718285</td>\n",
       "      <td>0.855973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLVQ</th>\n",
       "      <td>0.535978</td>\n",
       "      <td>0.395782</td>\n",
       "      <td>0.424137</td>\n",
       "      <td>0.363829</td>\n",
       "      <td>13.681207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.674523</td>\n",
       "      <td>0.820989</td>\n",
       "      <td>0.532343</td>\n",
       "      <td>0.569366</td>\n",
       "      <td>7.347104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.971160</td>\n",
       "      <td>0.959313</td>\n",
       "      <td>0.967903</td>\n",
       "      <td>0.963494</td>\n",
       "      <td>0.061970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.984574</td>\n",
       "      <td>0.990783</td>\n",
       "      <td>0.982197</td>\n",
       "      <td>0.986421</td>\n",
       "      <td>1.954252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.881958</td>\n",
       "      <td>0.943424</td>\n",
       "      <td>0.904685</td>\n",
       "      <td>0.921704</td>\n",
       "      <td>5.145838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPClassifier</th>\n",
       "      <td>0.783942</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>0.781311</td>\n",
       "      <td>0.800896</td>\n",
       "      <td>4.461666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VotingClassifier_dt_knn_rf</th>\n",
       "      <td>0.974514</td>\n",
       "      <td>0.981204</td>\n",
       "      <td>0.967318</td>\n",
       "      <td>0.974074</td>\n",
       "      <td>2.925196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VotingClassifier_mlp1_mlp2_mlp3</th>\n",
       "      <td>0.790265</td>\n",
       "      <td>0.840359</td>\n",
       "      <td>0.786036</td>\n",
       "      <td>0.809008</td>\n",
       "      <td>13.662096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 accuracy  precision    recall        f1   \n",
       "KNeighborsClassifier             0.749545   0.809206  0.665667  0.718285  \\\n",
       "GLVQ                             0.535978   0.395782  0.424137  0.363829   \n",
       "SVC                              0.674523   0.820989  0.532343  0.569366   \n",
       "DecisionTreeClassifier           0.971160   0.959313  0.967903  0.963494   \n",
       "RandomForestClassifier           0.984574   0.990783  0.982197  0.986421   \n",
       "XGBClassifier                    0.881958   0.943424  0.904685  0.921704   \n",
       "MLPClassifier                    0.783942   0.832500  0.781311  0.800896   \n",
       "VotingClassifier_dt_knn_rf       0.974514   0.981204  0.967318  0.974074   \n",
       "VotingClassifier_mlp1_mlp2_mlp3  0.790265   0.840359  0.786036  0.809008   \n",
       "\n",
       "                                  fit_time  \n",
       "KNeighborsClassifier              0.855973  \n",
       "GLVQ                             13.681207  \n",
       "SVC                               7.347104  \n",
       "DecisionTreeClassifier            0.061970  \n",
       "RandomForestClassifier            1.954252  \n",
       "XGBClassifier                     5.145838  \n",
       "MLPClassifier                     4.461666  \n",
       "VotingClassifier_dt_knn_rf        2.925196  \n",
       "VotingClassifier_mlp1_mlp2_mlp3  13.662096  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_results = pd.DataFrame(models_results).T.dropna()\n",
    "models_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model:  KNeighborsClassifier\n",
      "Training model:  GLVQ\n",
      "Training model:  SVC\n",
      "Training model:  DecisionTreeClassifier\n",
      "Training model:  RandomForestClassifier\n",
      "Training model:  XGBClassifier\n",
      "Training model:  MLPClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matheus\\Documents\\Git\\machine_learning_ufpe_2023_2\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model:  VotingClassifier_dt_knn_rf\n",
      "Training model:  VotingClassifier_mlp1_mlp2_mlp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matheus\\Documents\\Git\\machine_learning_ufpe_2023_2\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Matheus\\Documents\\Git\\machine_learning_ufpe_2023_2\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9min 23s\n",
      "Wall time: 9min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matheus\\Documents\\Git\\machine_learning_ufpe_2023_2\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Best models of a pre-run are chosen to ensemble\n",
    "models = [\n",
    "    KNeighborsClassifier(\n",
    "        n_neighbors=16, weights=\"distance\", metric=\"manhattan\", n_jobs=-1\n",
    "    ),\n",
    "    GLVQ(\n",
    "        activation_type=\"swish\",\n",
    "        distance_type=\"squared-euclidean\",\n",
    "        prototype_n_per_class=83,\n",
    "    ),\n",
    "    SVC(C=172.60, kernel=\"sigmoid\", degree=3),\n",
    "    DecisionTreeClassifier(\n",
    "        criterion=\"entropy\",\n",
    "        splitter=\"best\",\n",
    "        max_depth=16,\n",
    "        min_samples_split=17,\n",
    "        min_samples_leaf=6,\n",
    "    ),\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=15,\n",
    "        criterion=\"gini\",\n",
    "        max_depth=20,\n",
    "        min_samples_leaf=5,\n",
    "        min_samples_split=34,\n",
    "    ),\n",
    "    XGBClassifier(max_depth=31, learning_rate=8.98e-09, n_estimators=18, gamma=0.064),\n",
    "    MLPClassifier(\n",
    "        activation=\"tanh\",\n",
    "        solver=\"sgd\",\n",
    "        hidden_layer_sizes=[72],\n",
    "        learning_rate=\"adaptive\",\n",
    "        learning_rate_init=0.75,\n",
    "    ),\n",
    "    VotingClassifier(\n",
    "        estimators=[\n",
    "            (\n",
    "                \"dt\",\n",
    "                DecisionTreeClassifier(\n",
    "                    criterion=\"entropy\",\n",
    "                    splitter=\"best\",\n",
    "                    max_depth=16,\n",
    "                    min_samples_split=17,\n",
    "                    min_samples_leaf=6,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"knn\",\n",
    "                KNeighborsClassifier(\n",
    "                    n_neighbors=16, weights=\"distance\", metric=\"manhattan\", n_jobs=-1\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"rf\",\n",
    "                RandomForestClassifier(\n",
    "                    n_estimators=15,\n",
    "                    criterion=\"gini\",\n",
    "                    max_depth=20,\n",
    "                    min_samples_leaf=5,\n",
    "                    min_samples_split=34,\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "        voting=\"hard\",\n",
    "    ),\n",
    "    VotingClassifier(\n",
    "        estimators=[\n",
    "            (\n",
    "                \"mlp1\",\n",
    "                MLPClassifier(\n",
    "                    activation=\"tanh\",\n",
    "                    solver=\"sgd\",\n",
    "                    hidden_layer_sizes=72,\n",
    "                    learning_rate=\"adaptive\",\n",
    "                    learning_rate_init=0.75,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"mlp2\",\n",
    "                MLPClassifier(\n",
    "                    activation=\"tanh\",\n",
    "                    solver=\"sgd\",\n",
    "                    hidden_layer_sizes=[72],\n",
    "                    learning_rate=\"adaptive\",\n",
    "                    learning_rate_init=0.75,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"mlp3\",\n",
    "                MLPClassifier(\n",
    "                    activation=\"tanh\",\n",
    "                    solver=\"sgd\",\n",
    "                    hidden_layer_sizes=[72],\n",
    "                    learning_rate=\"adaptive\",\n",
    "                    learning_rate_init=0.75,\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "        voting=\"hard\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "models_results = {get_model_name(model): {} for model in models}\n",
    "\n",
    "for model in models:\n",
    "    print(\"Training model: \", get_model_name(model))\n",
    "\n",
    "    timer = Timer()\n",
    "    model.fit(X_train, y_train)\n",
    "    metrics = compute_metrics(y_test, model.predict(X_test))\n",
    "    metrics[\"fit_time\"] = timer()\n",
    "\n",
    "    # add new name to dataframe\n",
    "    models_results[get_model_name(model)] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.814027</td>\n",
       "      <td>0.890812</td>\n",
       "      <td>0.733634</td>\n",
       "      <td>0.790114</td>\n",
       "      <td>1.024093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLVQ</th>\n",
       "      <td>0.670308</td>\n",
       "      <td>0.535439</td>\n",
       "      <td>0.538546</td>\n",
       "      <td>0.498649</td>\n",
       "      <td>522.833991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.417840</td>\n",
       "      <td>0.246153</td>\n",
       "      <td>0.207907</td>\n",
       "      <td>0.216263</td>\n",
       "      <td>4.332048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.961387</td>\n",
       "      <td>0.921012</td>\n",
       "      <td>0.856274</td>\n",
       "      <td>0.845889</td>\n",
       "      <td>0.085969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.909648</td>\n",
       "      <td>0.949339</td>\n",
       "      <td>0.763796</td>\n",
       "      <td>0.803535</td>\n",
       "      <td>0.255509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.931110</td>\n",
       "      <td>0.957592</td>\n",
       "      <td>0.846116</td>\n",
       "      <td>0.858905</td>\n",
       "      <td>2.464781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPClassifier</th>\n",
       "      <td>0.905241</td>\n",
       "      <td>0.917129</td>\n",
       "      <td>0.907599</td>\n",
       "      <td>0.912234</td>\n",
       "      <td>2.889228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VotingClassifier_dt_knn_rf</th>\n",
       "      <td>0.928236</td>\n",
       "      <td>0.964599</td>\n",
       "      <td>0.844486</td>\n",
       "      <td>0.891990</td>\n",
       "      <td>1.366978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VotingClassifier_mlp1_mlp2_mlp3</th>\n",
       "      <td>0.934847</td>\n",
       "      <td>0.956159</td>\n",
       "      <td>0.943042</td>\n",
       "      <td>0.949329</td>\n",
       "      <td>8.716191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 accuracy  precision    recall        f1   \n",
       "KNeighborsClassifier             0.814027   0.890812  0.733634  0.790114  \\\n",
       "GLVQ                             0.670308   0.535439  0.538546  0.498649   \n",
       "SVC                              0.417840   0.246153  0.207907  0.216263   \n",
       "DecisionTreeClassifier           0.961387   0.921012  0.856274  0.845889   \n",
       "RandomForestClassifier           0.909648   0.949339  0.763796  0.803535   \n",
       "XGBClassifier                    0.931110   0.957592  0.846116  0.858905   \n",
       "MLPClassifier                    0.905241   0.917129  0.907599  0.912234   \n",
       "VotingClassifier_dt_knn_rf       0.928236   0.964599  0.844486  0.891990   \n",
       "VotingClassifier_mlp1_mlp2_mlp3  0.934847   0.956159  0.943042  0.949329   \n",
       "\n",
       "                                   fit_time  \n",
       "KNeighborsClassifier               1.024093  \n",
       "GLVQ                             522.833991  \n",
       "SVC                                4.332048  \n",
       "DecisionTreeClassifier             0.085969  \n",
       "RandomForestClassifier             0.255509  \n",
       "XGBClassifier                      2.464781  \n",
       "MLPClassifier                      2.889228  \n",
       "VotingClassifier_dt_knn_rf         1.366978  \n",
       "VotingClassifier_mlp1_mlp2_mlp3    8.716191  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_results = pd.DataFrame(models_results).T.dropna()\n",
    "models_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "def stratified_cross_val(model_list, data, n_folds=30, n_splits=10):\n",
    "    model_results = {get_model_name(model): [] for model in model_list}\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "    X = data.iloc[:, :-1]\n",
    "    y = LabelEncoder().fit_transform(data.iloc[:, -1])\n",
    "\n",
    "    for model in model_list:\n",
    "        pipeline = Pipeline(\n",
    "            [\n",
    "                # (\"scaler\", StandardScaler()), # uncomment to scale data\n",
    "                (\"model\", model)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for _ in range(n_folds):\n",
    "            iter_scores = cross_validate(\n",
    "                pipeline,\n",
    "                X,\n",
    "                y,\n",
    "                cv=cv,\n",
    "                scoring={\n",
    "                    \"f1_score\": \"f1_macro\",\n",
    "                    \"precision_score\": \"precision_macro\",\n",
    "                    \"accuracy_score\": \"accuracy\",\n",
    "                    \"recall_score\": \"recall_macro\",\n",
    "                },\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "            model_results[get_model_name(model)].append(iter_scores)\n",
    "\n",
    "    return model_results\n",
    "\n",
    "\n",
    "\n",
    "def model_results_to_df(model_results):\n",
    "    res_final = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"fit_time\",\n",
    "            \"score_time\",\n",
    "            \"test_f1_score\",\n",
    "            \"test_precision_score\",\n",
    "            \"test_accuracy_score\",\n",
    "            \"test_recall_score\",\n",
    "            \"split\",\n",
    "            \"model\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for model in model_results.keys():\n",
    "        for i in range(len(model_results[model])):\n",
    "            _res = pd.DataFrame(\n",
    "                model_results[model][i].values(),\n",
    "                index=model_results[model][i].keys(),\n",
    "            ).T\n",
    "            _res[\"split\"] = i\n",
    "            _res[\"model\"] = model\n",
    "            res_final = pd.concat([res_final, _res])\n",
    "\n",
    "    return res_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    KNeighborsClassifier(),\n",
    "    GLVQ(),\n",
    "    SVC(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    XGBClassifier(),\n",
    "    MLPClassifier(),\n",
    "    VotingClassifier(estimators=[('dt', DecisionTreeClassifier()), ('knn', KNeighborsClassifier()), ('rf', RandomForestClassifier())], voting = 'hard'),\n",
    "    VotingClassifier(estimators=[('mlp1', MLPClassifier()), ('mlp2', MLPClassifier()), ('mlp3', MLPClassifier()) ], voting = 'hard')\n",
    "]\n",
    "\n",
    "stratified_crossval_results = model_results_to_df(\n",
    "    stratified_cross_val(models, ds_train, n_folds=10, n_splits=5)\n",
    ")\n",
    "\n",
    "stratified_crossval_results.head().style"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_crossval_results.groupby(\"model\").mean(numeric_only=True).style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_crossval_results.groupby(\"model\").std(numeric_only=True).style"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "import os\n",
    "# Avoid use n_jobs=-1 in each call of sklearn model\n",
    "os.environ[\"JOBLIB_MULTIPROCESSING\"] = \"true\"\n",
    "\n",
    "def objective(trial, model_name, data):\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "\n",
    "    if model_name == \"KNeighborsClassifier\":\n",
    "        n_neighbors = trial.suggest_int(\"n_neighbors\", 1, 100)\n",
    "        weights = trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n",
    "        metric = trial.suggest_categorical(\"metric\", [\"euclidean\", \"manhattan\"])\n",
    "        model = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, metric=metric)\n",
    "\n",
    "    elif model_name == \"GLVQ\":\n",
    "        activation_type = trial.suggest_categorical(\"activation_type\", [\"identity\", \"sigmoid\", \"soft+\", \"swish\"])\n",
    "        distance_type = trial.suggest_categorical(\"distance_type\", [\"squared-euclidean\", \"euclidean\"])\n",
    "        prototype_n_per_class = trial.suggest_int(\"prototype_n_per_class\", 1, 100)\n",
    "        model = GLVQ(prototype_n_per_class=prototype_n_per_class, activation_type=activation_type, distance_type=distance_type, random_state=42)\n",
    "\n",
    "    elif model_name == \"SVC\":\n",
    "        c = trial.suggest_loguniform(\"c\", 1e-10, 1e10)\n",
    "        kernel = trial.suggest_categorical(\"kernel\", [\"poly\", \"rbf\", \"sigmoid\"])\n",
    "        degree = trial.suggest_int(\"degree\", 1, 5)\n",
    "        model = SVC(C=c, kernel=kernel, degree=degree)\n",
    "\n",
    "    elif model_name == \"DecisionTreeClassifier\":\n",
    "        criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n",
    "        splitter = trial.suggest_categorical(\"splitter\", [\"best\", \"random\"])\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 1, 100)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 100)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 100)\n",
    "        model = DecisionTreeClassifier(criterion=criterion, splitter=splitter, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "    \n",
    "    elif model_name == \"RandomForestClassifier\":\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 1, 25)\n",
    "        criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 1, 100)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 100)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 100)\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split)\n",
    "    \n",
    "    elif model_name == \"XGBClassifier\":\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 1, 100)\n",
    "        learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-10, 1e10)\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 1, 25)\n",
    "        gamma = trial.suggest_loguniform(\"gamma\", 1e-10, 1e10)\n",
    "        model = XGBClassifier(max_depth=max_depth, learning_rate=learning_rate, n_estimators=n_estimators, gamma=gamma)\n",
    "    \n",
    "    elif model_name == \"MLPClassifier\":\n",
    "        activation = trial.suggest_categorical(\"activation\", [\"identity\", \"logistic\", \"tanh\", \"relu\"])\n",
    "        solver = trial.suggest_categorical(\"solver\", [\"sgd\", \"adam\"])\n",
    "        n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "        hidden_layer_sizes = trial.suggest_int(\"hidden_layer_sizes\", 1, 100)\n",
    "        layers = [hidden_layer_sizes] * n_layers\n",
    "        learning_rate_type = trial.suggest_categorical(\"learning_rate_type\", [\"constant\", \"invscaling\", \"adaptive\"])\n",
    "        learning_rate_init = trial.suggest_loguniform(\"learning_rate_init\", 1e-10, 1e10)\n",
    "        model = MLPClassifier(activation=activation, solver=solver, hidden_layer_sizes=layers, learning_rate=learning_rate_type, learning_rate_init=learning_rate_init)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    return f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "models = [\n",
    "    KNeighborsClassifier(),\n",
    "    GLVQ(),\n",
    "    SVC(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    XGBClassifier(),\n",
    "    MLPClassifier(),\n",
    "    # VotingClassifier(estimators=[('dt', DecisionTreeClassifier()), ('knn', KNeighborsClassifier()), ('rf', RandomForestClassifier())], voting = 'hard'),\n",
    "    # VotingClassifier(estimators=[('mlp1', MLPClassifier()), ('mlp2', MLPClassifier()), ('mlp3', MLPClassifier()) ], voting = 'hard')\n",
    "]\n",
    "\n",
    "models_names = [get_model_name(model) for model in models]\n",
    "N_TRIALS = 10\n",
    "\n",
    "results = {model: {} for model in models_names}\n",
    "\n",
    "dataset_tunning = ds_train.sample(frac=0.4, random_state=42)\n",
    "X_train_tunning, X_test_tunning, y_train_tunning, y_test_tunning = train_test_split(dataset_tunning.iloc[:, :-1], dataset_tunning.iloc[:, -1], test_size=0.2, random_state=42)\n",
    "valid_split = (X_train_tunning, X_test_tunning, y_train_tunning, y_test_tunning)\n",
    "\n",
    "for model_name in models_names:\n",
    "    search = lambda trial: objective(trial, model_name=model_name, data=valid_split)\n",
    "    sampler = optuna.samplers.TPESampler(seed=42)\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "    study.optimize(search, n_trials=N_TRIALS)\n",
    "\n",
    "    print(\n",
    "        \"To model %s the best parameter is:\"\n",
    "        % (model_name)\n",
    "    )\n",
    "    print(study.best_params)\n",
    "    print(\"F1-score:%.3f\" % study.best_value, \"\\n\")\n",
    "\n",
    "    results[model_name] = (study.best_params, study.best_value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics average with confidence intervals\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "results_long = stratified_crossval_results.melt(\n",
    "    id_vars=[\"split\", \"model\"],\n",
    "    value_vars=[\n",
    "        \"test_f1_score\",\n",
    "        \"test_precision_score\",\n",
    "        \"test_accuracy_score\",\n",
    "        \"test_recall_score\",\n",
    "    ],\n",
    "    var_name=\"metric\",\n",
    "    value_name=\"score\",\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.barplot(\n",
    "    data=results_long, x=\"model\", y=\"score\", hue=\"metric\", errorbar=\"sd\", errwidth=2, capsize=0.05, palette=\"crest\")\n",
    "plt.title(\"Metrics by model\", fontsize=16)\n",
    "plt.ylabel(\"Score\", fontsize=14)\n",
    "plt.xlabel(\"Model\", fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.legend(title=\"Metric\", title_fontsize=\"14\", loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Friedman and Nemeyni Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import friedmanchisquare\n",
    "from scikit_posthocs import posthoc_nemenyi_friedman\n",
    "\n",
    "\n",
    "def perform_friedman_nemenyi_tests(df):\n",
    "    metrics = [\n",
    "        \"test_f1_score\",\n",
    "        \"test_precision_score\",\n",
    "        \"test_accuracy_score\",\n",
    "        \"test_recall_score\",\n",
    "    ]\n",
    "    models = df[\"model\"].unique()\n",
    "    folds = df[\"split\"].unique()\n",
    "\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        results[metric] = np.zeros((len(models), len(folds)))\n",
    "        for i, model in enumerate(models):\n",
    "            for j, fold in enumerate(folds):\n",
    "                results[metric][i, j] = df.loc[\n",
    "                    (df[\"model\"] == model) & (df[\"split\"] == fold)\n",
    "                ][metric].values[0]\n",
    "\n",
    "    # Friedman test\n",
    "    for metric in metrics:\n",
    "        stat, p_value = friedmanchisquare(*results[metric])\n",
    "        print(f\"{metric}: Friedman Chi Square = {stat:.3f}, p-value = {p_value:.3f}\")\n",
    "        if p_value < 0.05:\n",
    "            print(\"Significant difference between models\\n\")\n",
    "        else:\n",
    "            print(\"No significant difference between models\")\n",
    "\n",
    "    # Nemenyi test\n",
    "    for metric in metrics:\n",
    "        posthoc = posthoc_nemenyi_friedman(results[metric].T)\n",
    "        print(f\"\\nNemenyi PostHoc Test for {metric}:\")\n",
    "        print(posthoc)\n",
    "\n",
    "\n",
    "perform_friedman_nemenyi_tests(stratified_crossval_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box-Plot Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse models with boxplots\n",
    "\n",
    "# metrics to be analyzed\n",
    "metrics = ['fit_time', 'score_time', 'test_f1_score', 'test_precision_score', 'test_accuracy_score', 'test_recall_score']\n",
    "\n",
    "# create a separate plot for each metric\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(13, 5))\n",
    "    sns.boxplot(x='model', y=metric, data=stratified_crossval_results)\n",
    "    plt.title(f'Boxplot of {metric} by model')\n",
    "    plt.xticks()  # rotate x-axis labels\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models for comparison (when hyperparameters are tunned, pass tunning params to the model)\n",
    "\n",
    "models = [\n",
    "    KNeighborsClassifier(),\n",
    "    GLVQ(),\n",
    "    SVC(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    XGBClassifier(),\n",
    "    MLPClassifier(),\n",
    "]\n",
    "\n",
    "models_results = {get_model_name(model): {} for model in models}\n",
    "\n",
    "for model in models:\n",
    "    print(\"Training model: \", get_model_name(model))\n",
    "\n",
    "    timer = Timer()\n",
    "    model.fit(X_train, y_train)\n",
    "    metrics = compute_metrics(y_test, model.predict(X_test))\n",
    "    metrics[\"fit_time\"] = timer()\n",
    "    models_results[get_model_name(model)] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(model, X_test, y_test, normalize=None, ax=None):\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize=normalize)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    ax.imshow(cm, cmap=plt.cm.Blues)\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            if normalize == 'true':\n",
    "                text = f'{cm[i, j]:.2f}'\n",
    "            else:\n",
    "                text = str(cm[i, j])\n",
    "            ax.text(j, i, text, ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > cm.max()/2 else \"black\")\n",
    "    \n",
    "    # fig.delaxes(axs[-1, -1])\n",
    "\n",
    "    ax.set_xticks(range(len(label_encoder.classes_)))\n",
    "    ax.set_xticklabels(label_encoder.classes_, rotation=90)\n",
    "    ax.set_yticks(range(len(label_encoder.classes_)))\n",
    "    ax.set_yticklabels(label_encoder.classes_)\n",
    "    ax.set_xlabel(\"Predicted label\")\n",
    "    ax.set_ylabel(\"True label\")\n",
    "    ax.set_title(f\"Confusion Matrix for {get_model_name(model)}\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "for i, model in enumerate(models):\n",
    "    plot_confusion_matrix(model, X_test, y_test, ax=axes[i // 4, i % 4])\n",
    "plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
